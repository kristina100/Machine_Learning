# 集成学习

也称多分类学习

Bootstrap Sample ---  有放回的取样 --- 可以得到不一致的Sample

![image-20210803091235549](C:/Users/HUAWEI/AppData/Roaming/Typora/typora-user-images/image-20210803091235549.png)

##   Bagging (Bootstrap  Sample)



* 有放回的取样，训练分类器
* 分类器投票
* 每一个分类器都有输出，输出的权重是一样的
*  ![image-20210803091741818](C:/Users/HUAWEI/AppData/Roaming/Typora/typora-user-images/image-20210803091741818.png)

## Random Forests



![image-20210803092344369](C:/Users/HUAWEI/AppData/Roaming/Typora/typora-user-images/image-20210803092344369.png)

* 首先明确一点：多分类器决策一定是每个分类器都有所不同
* 一般理解是，多分类器使用不同的分类器，但是最常用的是使用同一类型的分类器，比如说随机森林，每个分类器都是决策树，那么我们要做的一点就是使得出的决策树不一样。

![image-20210803092841158](C:/Users/HUAWEI/AppData/Roaming/Typora/typora-user-images/image-20210803092841158.png)

上述算法是如何来实现是决策树不一样呢~

* 首先特征决策时，是随机挑选一些特征，而不是全部遍历

* 其次，只是用了2/3的数据集，随机留下1/3当作测试集

* 决策树的数量最少500

* 不需要担心过学习问题，因为聚集的决策树越多，一些特殊的过学习决策树对整体结果影响不大

  

## Stacking

stacking是把RF的输出，再放到分类器进行学习之后再输出

* 分类器同时生成，彼此之间没有什么关系

![image-20210803093909646](C:/Users/HUAWEI/AppData/Roaming/Typora/typora-user-images/image-20210803093909646.png)

## Boosting

* 分类器生成中，彼此都有影响。
* 环环相扣

## AbdBoost

![image-20210803100733843](C:/Users/HUAWEI/AppData/Roaming/Typora/typora-user-images/image-20210803100733843.png)

##### The choice of \alpha

![image-20210803103903173](C:/Users/HUAWEI/AppData/Roaming/Typora/typora-user-images/image-20210803103903173.png)

![image-20210803104540439](C:/Users/HUAWEI/AppData/Roaming/Typora/typora-user-images/image-20210803104540439.png)

## ![image-20210803104511799](C:/Users/HUAWEI/AppData/Roaming/Typora/typora-user-images/image-20210803104511799.png)





## REgionBoost

出现了动态权重

![image-20210803104357952](C:/Users/HUAWEI/AppData/Roaming/Typora/typora-user-images/image-20210803104357952.png)

## CART 分类与回归树

决策树主要分为两种类型：

* 分类树的输出是样本的类标
* 回归树的输出是一个实数
* CART与ID3区别： CART中用于选择变量的不纯性度量是Gini指数； 如果目标变量是标称的，并且是具有两个以上的类别，则CART可能考虑将目标类别合并成两个超类别（双化）； 如果目标变量是连续的，则CART算法找出一组基于树的回归方程来预测目标变量。

##### 构建决策树

* 构建决策树时通常采用自上而下的方法，在每一步选择一个最好的属性来分裂
*  "最好" 的定义是使得子节点中的训练集尽量的纯

有4中不同的不纯度量可以用来发现CART模型的划分

取决于目标变量的类型，对于分类的目标变量，可以选择GINI，双化或有序双化； 对于连续的目标变量，可以使用最小二乘偏差（LSD）或最小绝对偏差（LAD）

##### GINI指数

GINI指数： 

1、是一种不等性度量； 

2、通常用来度量收入不平衡，可以用来度量任何不均匀分布； 

3、是介于0~1之间的数，0-完全相等，1-完全不相等； 

4、总体内包含的类别越杂乱，GINI指数就越大（跟熵的概念很相似）。



