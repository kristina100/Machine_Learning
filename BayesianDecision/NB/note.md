

# 贝叶斯分类器

## 一、 贝叶斯决策论

### 1.1  朴素贝叶斯分类

#### 1.1.3 核心算法：

​	
$$
P(B|A) = \frac{P(A|B)P(B)}{P(A)}\\
p(类别|特征)=\frac{P(特征|类别)p(类别)}{p(特征)}
$$

#### 1.1.2理解原理：

 * 概率问题三要素：**样本空间，事件，概率**

   例子：经典的掷硬币

 * **样本空间**就是事件条件下所得到的所有结果，因此掷一次硬币的样本空间为{正面，反面}。

 * 而概率空间中的**事件**与我们平时生活中所说的事件没有任何分别。这里指的是此掷一次硬币。

 * **概率**，就是事件发生后，出现结果的个数与样本空间个数的比值。假如此次为掷硬币的结果为正面，因为正面这个结果只发生了一次，而样本空间的个数是2，所以掷一次硬币出现正面的概率就为1/2.

"A事件发生的情况下",代表A为样本空间，B"事件发生的概率",A并B代表为事件。

引例：

![](https://raw.githubusercontent.com/kristina100/Machine_Learning/master/BayesianDecision/picture/1.png)



![](https://raw.githubusercontent.com/kristina100/Machine_Learning/master/BayesianDecision/picture/2.png)

![](https://raw.githubusercontent.com/kristina100/Machine_Learning/master/BayesianDecision/picture/3.png)

因此
$$
P(D) = P(D\cap A)+P(D\cap B)+P(D\cap C)
$$
算出来的结果是事件D在样本空间S下发生的概率。

![](https://raw.githubusercontent.com/kristina100/Machine_Learning/master/BayesianDecision/picture/4.png)

因为要求的是M是由A发生的概率

所以先发生A再发生D

![](https://raw.githubusercontent.com/kristina100/Machine_Learning/master/BayesianDecision/picture/5.png)

计算事件在样本空间下的概率

![](https://raw.githubusercontent.com/kristina100/Machine_Learning/master/BayesianDecision/picture/6.png)

那么M发生在A中的概率：
$$
P(A|D)=\frac{P(A\cap D)}{P(D)}\\
=\frac{P(D|A)P(A)}{P(D|A)P(A)+P(D|B)P(B)+P(D|C)P(AC)}
$$

  ## 二、似然估计

### 2.1 最大似然估计

​	理解：通过事实，推断出最有可能的硬币情况，就是最大似然估计

​	假设硬币的参数，比如抛10次硬币，有六次是花的，那么可以设参数为0.6，原始设定参数是0.5。看图，更换参数，得出概率的正态分布。所以似然函数是推测参数的分布。而求最大似然估计的问题，就变成了求似然函数的极值。在这里，极值出现在0.6。



​	经过多次实验进行最大似然估计
$$
用x_1,x_2,...,x_n表示每次的实验结果，因为每次实验都是独立的，所以似然函数写作\\
L(\theta)=f(x_1|\theta)f(x_2|\theta)...f(x_n|\theta)\\
f(x_n|\theta)表示在同一个参数下的实验结果，即条件概率
$$

### 2.2 极大拟然估计

​	寻找最大化参数

## 三、朴素贝叶斯分类器	

### 3.1  公式

 * 属性条件假设独立

   ​	
   $$
   P(B|A) = \frac{P(A|B)P(B)}{P(A)}=\frac{P(c)}{P(x)}\prod_{i=1}^dP(x_i|c)\\
   d是属性数目，x_i是x在第i个属性上的取值
   $$
   
 * 朴素贝叶斯分类器表达式

 * $$
   h_{nb}(x)=arg\max_{c\in y}P(c)\prod_{i=1}^dP(x_i|c)
   $$

   

 * 求解关键

   D_c表示训练集D中第c类样本组成的集合，若有充足的独立同分布样本，可先估计出先验概率：
   $$
   P(c)=\frac{|D_c|}{|D|}
   $$
   对于离散属性，令D_{c,x_i}表示D_c中第i个属性上取值为x_i的样本组成的集合，则条件概率：
   $$
   P(x_i|c)=\frac{|D_{c_i,x_i}|}{|D_c|}
   $$
   对于连续属性
   $$
   P(x_i|c)=\frac{1}{\sqrt{2\pi}\sigma_{c,i}}exp(-\frac{(x_i-\mu_{c,i})^2}{2\sigma_{c,i}^2})\\
   \mu_{c,i}和\sigma_{c,i}^2是第c类样本在第i个属性上取值的均差和方差
   $$

## 四、半朴素贝叶斯分类器

​	现实生活中很难做到分类时各个属性间相互独立，因此适当考虑一部分属性间的相互依赖关系，这种放低要求的分类称为半朴素贝叶斯。

​	最常用策略：假定每个属性最多依赖于其他最多一个属性，称其依赖的这个属性为超夫属性：独依赖估计（ODE）。

​	对比：

​	对某个样本的预测朴素贝叶斯公式：
$$
h(x)=\max (P(x)\prod_{i=1}^dP(x_i|c))
$$
​	修正为半朴素贝叶斯公式：x依赖于分类c和一个依赖属性pa_i

​	
$$
h(x)=\max (P(x)\prod_{i=1}^dP(x_i|c,pa_i))
$$

**拉普拉斯修正**

防止出现因为训练集中没有出现过的单词被赋为0，造成对分类有影响，只需要对每个类别下所有的划分计数加一

## 五、贝叶斯网

略

## 六、 EM算法

### 6.1 核心思想

​	分为两步：Expection-Step 和 Maximization-Step

 * E - Step 主要是通过观察数据和现有模型来估计参数，然后用这个估计的参数来计算似然函数的期望值
 * M - Step 寻找似然函数最大化时对应的参数

​    隐变量：未知变量，估计可得

### 6.2 例子

如果说例子 A 需要计算你可能没那么直观，那就举更一个简单的例子：

现在一个班里有 50 个男生和 50 个女生，且男女生分开。我们假定男生的身高服从正态分布 ，女生的身高则服从另一个正态分布 。这时候我们可以用极大似然法（MLE），分别通过这 50 个男生和 50 个女生的样本来估计这两个正态分布的参数。

但现在我们让情况复杂一点，就是这 50 个男生和 50 个女生混在一起了。我们拥有 100 个人的身高数据，却不知道这 100 个人每一个是男生还是女生。

这时候情况就有点尴尬，因为通常来说，我们只有知道了精确的男女身高的正态分布参数我们才能知道每一个人更有可能是男生还是女生。但从另一方面去考量，我们只有知道了每个人是男生还是女生才能尽可能准确地估计男女各自身高的正态分布的参数。

这个时候有人就想到我们**必须从某一点开始**，并**用迭代的办法**去解决这个问题：我们**先设定**男生身高和女生身高分布的**几个参数**（初始值），然后根据这些参数去**判断每一个样本（人）是男生还是女生**，之后根据标注后的样本再反过来**重新估计参数**。之后再多次重复这个过程，直至稳定。这个算法也就是 EM 算法。

