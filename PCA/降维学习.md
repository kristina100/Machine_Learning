# 降维学习

 lazy learning 在训练阶段只把样本保存起来

eager learning 在训练阶段就对样本进行处理

#### 最邻近分类器出错概率

$$
P(err) = 1-\sum_{c\in y}P(c|x)P(c|z)
$$



## 低维嵌入

### 多维缩放

要求原始空间中样本之间的距离在低维空间中得已保持                                                   





## 一、PCA

PCA（Principal Component Analysis） 是一种常见的数据分析方式，常用于高维数据的降维，可用于提取数据的主要特征分量

PCA 的数学推导可以从最大可分型和最近重构性两方面进行，前者的优化条件为划分后方差最大，后者的优化条件为点到划分平面距离最小，这里将从最大可分性的角度进行证明。

#### 1.  基变换的矩阵表示

![image-20210803154821231](http://kristina.oss-cn-hangzhou.aliyuncs.com/img/image-20210803154821231.png)

#### 2.  最大可分性

选择不同的基可以对同样一组数据给出不同的表示，如果基的数量少于向量本身的维数，则可以达到降维的效果，那么如何寻找这个基。使原始数据最好的保留。

##### 2.1 方差

我们知道数值的分散程度，可以用数学上的方差来表述。一个变量的方差可以看做是每个元素与变量均值的差的平方和的均值
$$
Var(a)=\frac{1}{m}\sum_{i=1}^{m}(a_i-\mu)^2
$$
为了方便处理，我们将每个变量的均值都化为 0 ，因此方差可以直接用每个元素的平方和除以元素个数表示:
$$
Var(a)=\frac{1}{m}\sum_{i=1}^{m}(a_i)^2
$$
于是上面的问题被形式化表述为：**寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大。**

##### 2.2 协方差

在一维空间中我们可以用方差来表示数据的分散程度，

而对于高维数据，我们用协方差进行约束，

协方差可以表示两个变量的相关性，

为了让两个变量尽可能表示更多的原始信息，我们希望它们之间不存在线性相关性，因为相关性意味着两个变量不是完全独立，必然存在重复表示的信息。

协方差公式：
$$
Cov(a,b)=\frac{1}{m-1}\sum_{i=1}^m(a_i-\mu_a)(b_i-\mu_b)
$$
由于均值为0，那么可以表示为：
$$
Cov(a,b)=\frac{1}{m}\sum_{i=1}^ma_ib_i
$$
样本数较大时，不必在意其是 m 还是 m-1，为了方便计算，我们分母取 m。

当协方差为 0 时，表示两个变量线性不相关。

为了让协方差为 0，我们选择第二个基时只能在与第一个基正交的方向上进行选择，因此最终选择的两个方向一定是正交的。

得到了降维问题的优化目标：**将一组 N 维向量降为 K 维，其目标是选择 K 个单位正交基，使得原始数据变换到这组基上后，各变量两两间协方差为 0，而变量方差则尽可能大（在正交的约束下，取最大的 K 个方差）**

##### 2.3 协方差矩阵

假设我们只有 a 和 b 两个变量，那么我们将它们按行组成矩阵 X：

![image-20210803154835410](http://kristina.oss-cn-hangzhou.aliyuncs.com/img/image-20210803154835410.png)

![image-20210803154901493](http://kristina.oss-cn-hangzhou.aliyuncs.com/img/image-20210803154901493.png)

**设我们有 m 个 n 维数据记录，将其排列成矩阵**X_(n,m)，设**则 C=1/mXX^T 是一个对称矩阵，其对角线分别对应各个变量的方差，而第 i 行 j 列和 j 行 i 列元素相同，表示 i 和 j 两个变量的协方差**。

##### 2.4 矩阵对角化

**我们需要将除对角线外的其它元素化为 0，并且在对角线上将元素按大小从上到下排列（变量方差尽可能大）**

设原始数据矩阵 X 对应的协方差矩阵为 C，而 P 是一组基按行组成的矩阵，设 Y=PX，则 Y 为 X 对 P 做基变换后的数据。设 Y 的协方差矩阵为 D：那么C与D的关系

![image-20210803160358686](http://kristina.oss-cn-hangzhou.aliyuncs.com/img/image-20210803160358686.png)

优化目标变成了**寻找一个矩阵 P，满足PCP^T是一个对角矩阵，并且对角元素按从大到小依次排列，那么 P 的前 K 行就是要寻找的基，用 P 的前 K 行组成的矩阵乘以 X 就使得 X 从 N 维降到了 K 维并满足上述优化条件**

由上文知道，协方差矩阵 C 是一个是对称矩阵，在线性代数中实对称矩阵有一系列非常好的性质：

1. 实对称矩阵不同特征值对应的特征向量必然正交。
2. 设特征向量\lambda 重数为 r，则必然存在 r 个线性无关的特征向量对应于 \lambda，因此可以将这 r 个特征向量单位正交化。

![image-20210803160713174](http://kristina.oss-cn-hangzhou.aliyuncs.com/img/image-20210803160713174.png)

![image-20210803160851348](http://kristina.oss-cn-hangzhou.aliyuncs.com/img/image-20210803160851348.png)




##### 2.5 拉格朗日乘子法

在叙述求协方差矩阵对角化时，我们给出希望变化后的变量有：**变量间协方差为 0 且变量内方差尽可能大**。然后我们通过实对称矩阵的性质给予了推导，此外我们还可以把它转换为最优化问题利用拉格朗日乘子法来给予推导。

![image-20210803161633292](http://kristina.oss-cn-hangzhou.aliyuncs.com/img/image-20210803161633292.png)



![image-20210803161652321](http://kristina.oss-cn-hangzhou.aliyuncs.com/img/image-20210803161652321.png)





## 二、PCA求解步骤

设有m条n维数据

1. 将原始数据按列组成n行m列矩阵X

2. 将X的每一行进行零均值化，即减去这一行的均值

3. 求出协方差矩阵
   $$
   C=\frac{1}{m}XX^T
   $$

4. 求出协方差矩阵的特征值以及对应的特征向量

5. 将特征向量按对应特征值大小从上到下按行排列成矩阵，取前K行组成矩阵P

6. Y=PX即为降维到K为后的数据

### 性质

1. **缓解维度灾难**：PCA 算法通过舍去一部分信息之后能使得样本的采样密度增大（因为维数降低了），这是缓解维度灾难的重要手段
2. **降噪**： 当数据受到噪声影响时，最小特征值对应的特征向量往往与噪声有关，将他们舍弃再一定程度上起到降噪的效果
3. **过拟合**： PCA保留了主要信息，但这个主要信息知识针对训练集，而且这个主要信息未必时重要信息。有可能舍弃了一些看似无用的信息，但是这些看似无用的信息恰好是重要信息，只是在训练集上没有很大的表现，所以 PCA 也可能加剧了过拟合
4. **特征独立**：PCA 不仅将数据压缩到低维，它也使得降维之后的数据各特征相互独立；

### 细节

#####  零均值化

当对训练集进行 PCA 降维时，也需要对验证集、测试集执行同样的降维。而**对验证集、测试集执行零均值化操作时，均值必须从训练集计算而来**，不能使用验证集或者测试集的中心向量。



## 三、核化线性降维

`PCA`方法假设从高维空间到低维空间的映射是线性的，但是在不少现实任务中可能需要非线性映射才能找到合适的低维空间来降维。

非线性降维的一种常用方法是基于核技巧对线性降维方法进行核化`kernelized`， 如核主成分分析`Kernelized PCA:KPCA` ，它是对`PCA`的一种推广。







